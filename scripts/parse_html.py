"""
Serendipity FM - HTML è§£æè„šæœ¬
ä»å¾®ä¿¡å…¬ä¼—å·ä¿å­˜çš„ HTML æ–‡ä»¶ä¸­æå–æ–‡ç« ä¿¡æ¯
"""

import os
import re
import json
import hashlib
import urllib.request
from bs4 import BeautifulSoup
from pathlib import Path


# éœ€è¦è·³è¿‡çš„å¼•å¯¼å›¾å…³é”®è¯ï¼ˆå…¬ä¼—å·é¡¶éƒ¨å¼•å¯¼å…³æ³¨çš„å›¾ç‰‡ï¼‰
SKIP_IMAGE_KEYWORDS = [
    'é£é‡Œé›¨é‡Œ', 'æ¯å¤©ç­‰ä½ ', 'ç‚¹å‡»ä¸Šæ–¹', 'è®¾ä¸ºæ˜Ÿæ ‡', 'å…³æ³¨æˆ‘ä»¬',
    'äºŒç»´ç ', 'qrcode', 'logo', 'banner', 'å…¬ä¼—å·'
]

# éœ€è¦è·³è¿‡çš„å›¾ç‰‡ URL æ¨¡å¼
SKIP_IMAGE_PATTERNS = [
    r'wx_fmt=gif',  # è·³è¿‡ GIF åŠ¨å›¾
    r'tp=webp',     # æŸäº›å°å›¾æ ‡
]

# ä¸­æ–‡è¯­éŸ³è§’è‰²æ± ï¼ˆEdge TTSï¼‰
VOICE_POOL = [
    "zh-CN-XiaoxiaoNeural",   # å¥³å£° - æ´»æ³¼
    "zh-CN-XiaoyiNeural",     # å¥³å£° - æ¸©æŸ”
    "zh-CN-YunjianNeural",    # ç”·å£° - æˆç†Ÿ
    "zh-CN-YunxiNeural",      # ç”·å£° - å¹´è½»
    "zh-CN-YunxiaNeural",     # ç”·å£° - å°‘å¹´
    "zh-CN-XiaohanNeural",    # å¥³å£° - çŸ¥æ€§ï¼ˆæ›¿æ¢XiaochenNeuralï¼‰
]


def get_voice_for_article(article_id: str) -> str:
    """æ ¹æ®æ–‡ç« IDç¡®å®šæ€§åœ°åˆ†é…ä¸€ä¸ªè¯­éŸ³è§’è‰²"""
    hash_val = int(hashlib.md5(article_id.encode()).hexdigest(), 16)
    return VOICE_POOL[hash_val % len(VOICE_POOL)]


def should_skip_image(img_url: str, img_alt: str = "") -> bool:
    """åˆ¤æ–­å›¾ç‰‡æ˜¯å¦åº”è¯¥è¢«è·³è¿‡"""
    # æ£€æŸ¥ alt æ–‡æœ¬
    alt_lower = img_alt.lower() if img_alt else ""
    for keyword in SKIP_IMAGE_KEYWORDS:
        if keyword in alt_lower:
            return True
    
    # æ£€æŸ¥ URL æ¨¡å¼
    for pattern in SKIP_IMAGE_PATTERNS:
        if re.search(pattern, img_url, re.IGNORECASE):
            return True
    
    return False


def extract_content_image(soup: BeautifulSoup) -> str | None:
    """
    ä»æ­£æ–‡ä¸­æå–åˆé€‚çš„å°é¢å›¾
    è·³è¿‡é¡¶éƒ¨å¼•å¯¼å›¾ï¼Œé€‰æ‹©æ­£æ–‡ä¸­æœ‰æ„ä¹‰çš„é…å›¾
    """
    # æŸ¥æ‰¾æ­£æ–‡å®¹å™¨
    content_div = soup.find('div', id='js_content')
    if not content_div:
        content_div = soup.find('div', class_='rich_media_content')
    
    if not content_div:
        return None
    
    # è·å–æ‰€æœ‰å›¾ç‰‡
    images = content_div.find_all('img')
    
    for img in images:
        # è·å–å›¾ç‰‡ URLï¼ˆå¯èƒ½åœ¨ä¸åŒå±æ€§ä¸­ï¼‰
        img_url = img.get('data-src') or img.get('src') or ''
        img_alt = img.get('alt', '')
        
        if not img_url or img_url.startswith('data:'):
            continue
        
        # è·³è¿‡å¼•å¯¼å›¾
        if should_skip_image(img_url, img_alt):
            continue
        
        # æ£€æŸ¥å›¾ç‰‡å°ºå¯¸ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        width = img.get('data-w') or img.get('width')
        if width:
            try:
                w = int(width)
                if w < 200:  # è·³è¿‡å¤ªå°çš„å›¾ç‰‡
                    continue
            except ValueError:
                pass
        
        return img_url
    
    return None


def extract_author(soup: BeautifulSoup, text_content: str) -> str:
    """
    æå–ä½œè€…ä¿¡æ¯
    ä¼˜å…ˆä»æ­£æ–‡å¼€å¤´æå–ï¼Œå¤‡é€‰ meta æ ‡ç­¾
    """
    # å°è¯•ä»æ­£æ–‡å¼€å¤´åŒ¹é…ä½œè€…ï¼ˆå¸¸è§æ ¼å¼ï¼‰
    author_patterns = [
        r'^([A-Za-z\u4e00-\u9fa5]{1,10})\s+æ¯æ—¥è±†ç“£',  # "K æ¯æ—¥è±†ç“£"
        r'ä½œè€…[ï¼š:]\s*([A-Za-z\u4e00-\u9fa5]{1,20})',
        r'æ–‡[/ï¼]\s*([A-Za-z\u4e00-\u9fa5]{1,20})',
        r'by\s+([A-Za-z\u4e00-\u9fa5]{1,20})',
    ]
    
    # åªæ£€æŸ¥å‰500ä¸ªå­—ç¬¦
    head_text = text_content[:500]
    
    for pattern in author_patterns:
        match = re.search(pattern, head_text, re.IGNORECASE)
        if match:
            return match.group(1).strip()
    
    # å°è¯•æŸ¥æ‰¾ç‰¹å®šçš„ä½œè€…å…ƒç´ 
    author_elem = soup.find('span', class_='rich_media_meta_text')
    if author_elem:
        author_text = author_elem.get_text(strip=True)
        # ä¿®å¤ï¼šå»é™¤é‡å¤çš„ä½œè€…åï¼ˆæœ‰æ—¶ä¼šé‡å¤ä¸¤æ¬¡ï¼‰
        if len(author_text) > 2:
            half_len = len(author_text) // 2
            if author_text[:half_len] == author_text[half_len:]:
                author_text = author_text[:half_len]
        return author_text
    
    # å¤‡é€‰ï¼šä» meta æ ‡ç­¾è·å–
    meta_author = soup.find('meta', attrs={'name': 'author'})
    if meta_author:
        author = meta_author.get('content', '')
        if author and author != 'è±†ç“£ç”¨æˆ·':
            return author
    
    return "ä½šå"


def extract_title(soup: BeautifulSoup) -> str:
    """æå–æ–‡ç« æ ‡é¢˜"""
    # ä¼˜å…ˆä» og:title è·å–
    og_title = soup.find('meta', property='og:title')
    if og_title:
        return og_title.get('content', '').strip()
    
    # å¤‡é€‰ï¼šä» title æ ‡ç­¾è·å–
    title_tag = soup.find('title')
    if title_tag:
        return title_tag.get_text(strip=True)
    
    return "æœªå‘½åæ–‡ç« "


def extract_description(soup: BeautifulSoup) -> str:
    """æå–æ–‡ç« æè¿°"""
    meta_desc = soup.find('meta', attrs={'name': 'description'})
    if meta_desc:
        return meta_desc.get('content', '').strip()
    
    og_desc = soup.find('meta', property='og:description')
    if og_desc:
        return og_desc.get('content', '').strip()
    
    return ""


def clean_text_for_tts(text: str) -> str:
    """
    æ¸…ç†æ–‡æœ¬ï¼Œä½¿å…¶é€‚åˆ TTS æœ—è¯»
    """
    # ç§»é™¤å¤šä½™çš„ç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    
    # ç§»é™¤ç‰¹æ®Šæ ‡è®°
    text = re.sub(r'ç‚¹å‡»ä¸Šæ–¹.*?è®¾ä¸ºæ˜Ÿæ ‡', '', text)
    text = re.sub(r'æœ¬æ–‡æ¥è‡ªè±†ç“£.*?åŸåˆ›å†…å®¹', '', text)
    text = re.sub(r'æ„Ÿè°¢ä½œè€…ä¸ºè±†ç“£æä¾›ä¼˜è´¨åŸåˆ›å†…å®¹', '', text)
    text = re.sub(r'ç”±è±†ç“£ç”¨æˆ·.*?æˆæƒå‘å¸ƒ', '', text)
    text = re.sub(r'åŸæ–‡æ ‡é¢˜[ï¼š:][^\n]+', '', text)
    
    # ç§»é™¤ URL
    text = re.sub(r'https?://\S+', '', text)
    
    # ç§»é™¤è¡¨æƒ…ç¬¦å·ï¼ˆä¿ç•™å¸¸è§ä¸­æ–‡æ ‡ç‚¹ï¼‰
    text = re.sub(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]', '', text)
    
    # ç§»é™¤å¤šä½™çš„æ ‡ç‚¹
    text = re.sub(r'[ã€‚]{2,}', 'ã€‚', text)
    text = re.sub(r'[ï¼]{2,}', 'ï¼', text)
    text = re.sub(r'[ï¼Ÿ]{2,}', 'ï¼Ÿ', text)
    
    return text.strip()


def extract_text_content(soup: BeautifulSoup) -> str:
    """æå–æ­£æ–‡çº¯æ–‡æœ¬å†…å®¹"""
    content_div = soup.find('div', id='js_content')
    if not content_div:
        content_div = soup.find('div', class_='rich_media_content')
    
    if not content_div:
        return ""
    
    # è·å–çº¯æ–‡æœ¬
    text = content_div.get_text(separator='\n', strip=True)
    
    # æ¸…ç†æ–‡æœ¬
    text = clean_text_for_tts(text)
    
    return text


def parse_html_file(file_path: str) -> dict | None:
    """
    è§£æå•ä¸ª HTML æ–‡ä»¶ï¼Œæå–æ‰€æœ‰éœ€è¦çš„ä¿¡æ¯
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
        return None
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æå–å„é¡¹ä¿¡æ¯
    title = extract_title(soup)
    text_content = extract_text_content(soup)
    author = extract_author(soup, text_content)
    description = extract_description(soup)
    cover_url = extract_content_image(soup)
    
    if not text_content or len(text_content) < 100:
        print(f"âš ï¸ æ–‡ç« å†…å®¹å¤ªçŸ­ï¼Œè·³è¿‡: {title}")
        return None
    
    # ç”Ÿæˆå”¯ä¸€ ID
    file_name = os.path.basename(file_path)
    article_id = hashlib.md5(file_name.encode()).hexdigest()[:8]
    
    # åˆ†é…è¯­éŸ³è§’è‰²
    voice = get_voice_for_article(article_id)
    
    # æ¥æºå›ºå®šä¸º"æ¯æ—¥è±†ç“£"ï¼ˆå¯ä»¥åç»­æ ¹æ®ä¸åŒæ¥æºæ‰©å±•ï¼‰
    source = "æ¯æ—¥è±†ç“£"
    
    # ç»å…¸è¯­å¥ä½¿ç”¨ description
    quote = description if description else ""
    
    return {
        "id": article_id,
        "title": title,
        "author": author,
        "source": source,
        "quote": quote,
        "description": description,
        "text_content": text_content,
        "cover_url": cover_url,
        "voice": voice,
        "source_file": file_name,
        "audio_file": f"audio/{article_id}.mp3",
        "cover_file": f"covers/{article_id}.jpg" if cover_url else None
    }


def download_cover_image(url: str, save_path: str) -> bool:
    """ä¸‹è½½å°é¢å›¾ç‰‡"""
    try:
        # æ·»åŠ  User-Agent é¿å…è¢«æ‹¦æˆª
        req = urllib.request.Request(url, headers={
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        with urllib.request.urlopen(req, timeout=10) as response:
            with open(save_path, 'wb') as f:
                f.write(response.read())
        return True
    except Exception as e:
        print(f"âš ï¸ ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
        return False


def parse_all_html_files(source_dir: str, output_dir: str) -> list[dict]:
    """
    è§£æç›®å½•ä¸‹æ‰€æœ‰ HTML æ–‡ä»¶
    """
    html_files = list(Path(source_dir).glob("*.html"))
    
    if not html_files:
        print(f"âŒ åœ¨ {source_dir} ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ° HTML æ–‡ä»¶")
        return []
    
    print(f"ğŸ“‚ æ‰¾åˆ° {len(html_files)} ä¸ª HTML æ–‡ä»¶")
    
    episodes = []
    
    for html_file in html_files:
        print(f"\nğŸ“„ è§£æ: {html_file.name}")
        
        result = parse_html_file(str(html_file))
        
        if result:
            # ä¸‹è½½å°é¢å›¾
            if result['cover_url']:
                cover_path = os.path.join(output_dir, result['cover_file'])
                if download_cover_image(result['cover_url'], cover_path):
                    print(f"  âœ… å°é¢å›¾å·²ä¸‹è½½")
                else:
                    result['cover_file'] = None
            
            episodes.append(result)
            print(f"  âœ… æ ‡é¢˜: {result['title']}")
            print(f"  âœ… ä½œè€…: {result['author']}")
            print(f"  âœ… è¯­éŸ³: {result['voice']}")
            print(f"  âœ… å†…å®¹é•¿åº¦: {len(result['text_content'])} å­—")
    
    return episodes


def save_episodes_data(episodes: list[dict], output_path: str):
    """ä¿å­˜èŠ‚ç›®æ•°æ®åˆ° JSON æ–‡ä»¶"""
    # ç§»é™¤ text_content å­—æ®µï¼ˆå¤ªé•¿ï¼Œå•ç‹¬ä¿å­˜ï¼‰
    episodes_meta = []
    
    for ep in episodes:
        meta = {k: v for k, v in ep.items() if k != 'text_content'}
        meta['duration'] = 0  # ç¨åç”±éŸ³é¢‘ç”Ÿæˆè„šæœ¬æ›´æ–°
        episodes_meta.append(meta)
        
        # ä¿å­˜æ–‡æœ¬å†…å®¹åˆ°å•ç‹¬æ–‡ä»¶ï¼ˆä¾› TTS ä½¿ç”¨ï¼‰
        text_path = output_path.replace('episodes.json', f"texts/{ep['id']}.txt")
        os.makedirs(os.path.dirname(text_path), exist_ok=True)
        with open(text_path, 'w', encoding='utf-8') as f:
            f.write(ep['text_content'])
    
    # ä¿å­˜å…ƒæ•°æ®
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump({"episodes": episodes_meta}, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… èŠ‚ç›®æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")


def main():
    # é¡¹ç›®æ ¹ç›®å½•
    project_dir = Path(__file__).parent.parent
    
    # HTML æºæ–‡ä»¶ç›®å½•ï¼ˆç›´æ¥ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ï¼Œå› ä¸º HTML æ–‡ä»¶å°±åœ¨é‚£é‡Œï¼‰
    source_dir = project_dir
    
    # è¾“å‡ºç›®å½•
    output_dir = project_dir
    
    print("=" * 50)
    print("ğŸ™ï¸ Serendipity FM - HTML è§£æå™¨")
    print("=" * 50)
    
    # è§£ææ‰€æœ‰ HTML æ–‡ä»¶
    episodes = parse_all_html_files(str(source_dir), str(output_dir))
    
    if not episodes:
        print("\nâŒ æ²¡æœ‰æˆåŠŸè§£æä»»ä½•æ–‡ç« ")
        return
    
    # ä¿å­˜æ•°æ®
    output_path = os.path.join(output_dir, "data", "episodes.json")
    save_episodes_data(episodes, output_path)
    
    print(f"\nğŸ‰ æˆåŠŸè§£æ {len(episodes)} ç¯‡æ–‡ç« ï¼")
    print("ä¸‹ä¸€æ­¥ï¼šè¿è¡Œ generate_audio.py ç”Ÿæˆè¯­éŸ³")


if __name__ == "__main__":
    main()
